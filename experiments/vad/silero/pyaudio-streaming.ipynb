{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Resources to Help You Get Started with Pyaudio and Wave Files\n",
    "\n",
    "https://nbviewer.org/github/mgeier/python-audio/blob/master/audio-files/audio-files-with-wave.ipynb\n",
    "https://stackoverflow.com/questions/35970282/what-are-chunks-samples-and-frames-when-using-pyaudio\n",
    "https://www.youtube.com/watch?v=ZqpSb5p1xQo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda venv (python 3.10)\n",
    "\n",
    "# %pip install numpy\n",
    "# %pip install torch\n",
    "# %pip install matplotlib\n",
    "# %pip install torchaudio\n",
    "# %pip install soundfile\n",
    "# %pip install pyaudio\n",
    "# %pip install jupyterplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from jupyterplot import ProgressPlot\n",
    "from IPython.display import Audio, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, utils = torch.hub.load(\n",
    "    repo_or_dir='snakers4/silero-vad',\n",
    "    model='silero_vad',\n",
    "    force_reload=True\n",
    ")\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(model, inputs: torch.Tensor, sample_rate: int):\n",
    "    with torch.no_grad():\n",
    "        outs = model(inputs, sample_rate)\n",
    "    return outs.item()\n",
    "\n",
    "def int2float(sound):\n",
    "    abs_max = np.abs(sound).max()\n",
    "    sound = sound.astype('float32')\n",
    "    if abs_max > 0:\n",
    "        sound *= 1/32768\n",
    "    sound = sound.squeeze()  # depends on the use case\n",
    "    return sound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyaudio Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "\n",
    "# how many audio samples per 1s of audio\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# how many frames the pyaudio stream reads at a time\n",
    "FRAMES_PER_BUFFER = int(SAMPLE_RATE / 10)\n",
    "\n",
    "# how many samples VAD reads from the pyaudio stream per iteration\n",
    "# 256 for 8kHZ, 512 for 16kHZ\n",
    "VAD_NUM_SAMPLES = 512\n",
    "VAD_THRESHOLD = 0.85\n",
    "\n",
    "# in the final recorded clips, how many seconds of audio before and after the detected speech should be included\n",
    "BUFFER_SECONDS_BEFORE = 0.5\n",
    "BUFFER_SECONDS_AFTER = 1.5\n",
    "\n",
    "BUFFER_BEFORE_SIZE = int(SAMPLE_RATE * BUFFER_SECONDS_BEFORE)\n",
    "BUFFER_AFTER_SIZE = int(SAMPLE_RATE * BUFFER_SECONDS_AFTER)\n",
    "\n",
    "audio = pyaudio.PyAudio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an enhancement to plot the speech probabilities in real time I added the implementation below. In contrast to the simeple one, it records the audio until to stop the recording by pressing enter. While looking into good ways to update matplotlib plots in real-time, I found a simple libarary that does the job. https://github.com/lvwerra/jupyterplot It has some limitations, but works for this use case really well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_clips = []    # Stores the audio clips\n",
    "\n",
    "def start_recording():\n",
    "    stream = audio.open(\n",
    "        format=FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=SAMPLE_RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=FRAMES_PER_BUFFER\n",
    "    )\n",
    "\n",
    "    # it stores y-axis values for the progress plot\n",
    "    voiced_confidences = []\n",
    "    \n",
    "    current_clip = []  # Stores the ongoing audio clip\n",
    "    pre_buffer = []  # Buffer to store audio before VAD crosses threshold\n",
    "    post_buffer = []  # Buffer to capture audio after threshold is crossed\n",
    "\n",
    "    pp = ProgressPlot(\n",
    "        plot_names=[\"Silero VAD\", \"Audio Clips Recording\"],\n",
    "        line_names=[\"speech probabilities\", \"threshold\"],\n",
    "        line_colors=[\"blue\", \"green\"],\n",
    "        x_label=\"time (s)\",\n",
    "        x_iterator=False, # Manually control x-axis (to update time in seconds)\n",
    "        width=1000,\n",
    "        y_lim=[0, 1]\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    vad_triggered = False  # Tracks whether VAD has been triggered\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            audio_chunk = stream.read(VAD_NUM_SAMPLES)\n",
    "\n",
    "            # Convert audio to appropriate format\n",
    "            audio_int16 = np.frombuffer(audio_chunk, np.int16)\n",
    "            audio_float32 = int2float(audio_int16)\n",
    "            audio_tensor = torch.from_numpy(audio_float32)\n",
    "        \n",
    "            # Get VAD confidence\n",
    "            new_confidence = get_probs(model, audio_tensor, SAMPLE_RATE)\n",
    "            voiced_confidences.append(new_confidence)\n",
    "        \n",
    "            # Update plot with time in seconds\n",
    "            elapsed_time = time.time() - start_time\n",
    "            pp.update(elapsed_time, [[new_confidence, VAD_THRESHOLD], [int(vad_triggered), VAD_THRESHOLD]])\n",
    "            \n",
    "            # Keep pre-buffering last `BUFFER_SECONDS_BEFORE` seconds of audio\n",
    "            pre_buffer.append(audio_chunk)\n",
    "            if len(pre_buffer) > int(BUFFER_BEFORE_SIZE / VAD_NUM_SAMPLES):\n",
    "                pre_buffer.pop(0)\n",
    "\n",
    "            # VAD detection logic\n",
    "            if new_confidence > VAD_THRESHOLD:\n",
    "                # Start capturing audio clip when threshold is crossed\n",
    "                if not vad_triggered:\n",
    "                    current_clip = pre_buffer.copy()  # Start with pre-buffer\n",
    "                    vad_triggered = True\n",
    "\n",
    "                current_clip.append(audio_chunk)  # Append current chunk\n",
    "\n",
    "            elif vad_triggered:\n",
    "                # If audio drops below threshold, capture post-buffer\n",
    "                if len(post_buffer) < int(BUFFER_AFTER_SIZE / VAD_NUM_SAMPLES):\n",
    "                    post_buffer.append(audio_chunk)\n",
    "                    current_clip.append(audio_chunk)\n",
    "                else:\n",
    "                    # After capturing post-buffer, stop the clip\n",
    "                    audio_clips.append(b''.join(current_clip))  # Store the full clip\n",
    "                    vad_triggered = False  # Reset trigger\n",
    "                    current_clip = []  # Clear current clip\n",
    "                    post_buffer = []  # Clear post-buffer\n",
    "        \n",
    "        except KeyboardInterrupt as e:\n",
    "            # if post_buffer is not empty, means program was interrupted while capturing audio for a clip\n",
    "            if current_clip:\n",
    "                audio_clips.append(b''.join(current_clip))\n",
    "            \n",
    "            print('Recorded clips:', len(audio_clips))\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "    pp.finalize()\n",
    "    stream.stop_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_clips = []    # Reset the audio clips\n",
    "start_recording()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Recorded audio clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio_clips(audio_clips, sample_rate=SAMPLE_RATE):\n",
    "    for idx, clip in enumerate(audio_clips):\n",
    "        print(f\"Playing clip {idx+1}:\")\n",
    "        clip_int16 = np.frombuffer(clip, np.int16)\n",
    "        clip_float32 = int2float(clip_int16)\n",
    "        display(Audio(clip_float32, rate=sample_rate))\n",
    "\n",
    "play_audio_clips(audio_clips)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
